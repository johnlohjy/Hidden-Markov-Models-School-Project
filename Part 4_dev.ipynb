{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36eab25f",
   "metadata": {},
   "source": [
    "# Part 4 Dev: Design Challenge: Smoothing with Laplace Emission. Testing on ES and RU dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481a80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428462f0",
   "metadata": {},
   "source": [
    "#### Define helper function that creates a list of lists where each list contains only the states of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8096f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a list of lists where each list contains only the states of a sentence\n",
    "def sentence_creator_states(original):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for i in original:\n",
    "        #If it is not an empty line\n",
    "        if i!='':\n",
    "            #Append the state to sentence\n",
    "            parts = i.split(\" \")\n",
    "            state = parts[-1]\n",
    "            sentence.append(state)\n",
    "        #If it is an empty line\n",
    "        #The sentence is complete\n",
    "        else:\n",
    "            #Add the START and STOP state for computation later\n",
    "            sentence.insert(0, \"START\")\n",
    "            sentence.append(\"STOP\")\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            \n",
    "    #In the case of the last sentence where it the training set does not end with an empty line\n",
    "    if len(sentence)!=0:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9495333",
   "metadata": {},
   "source": [
    "#### Define function that estimates emission parameters with laplace smoothing with alpha on the unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac4d195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_parameters(training_set, k_value=1, alpha=1):\n",
    "    state_count = {}\n",
    "    state_observation_count = {}\n",
    "    estimated_emission_parameters = {}\n",
    "    #Get a set of the trained words\n",
    "    trained_words = set()\n",
    "    \n",
    "    #k_value: k occurences of generating observation #UNK# from any label y\n",
    "    \n",
    "    for i in range(len(training_set)):\n",
    "        #if its not a single empty line that separates sentences\n",
    "        if(len(training_set[i])!=0):\n",
    "            parts = training_set[i].split(\" \")\n",
    "            observation = ' '.join(parts[:len(parts)-1])\n",
    "            state = parts[-1]\n",
    "            \n",
    "            #Increment count(y->x)\n",
    "            if (observation,state) in state_observation_count:\n",
    "                state_observation_count[(observation,state)]+=1\n",
    "            else:\n",
    "                state_observation_count[(observation,state)]=1\n",
    "\n",
    "            #Increment count(y)\n",
    "            if state in state_count:\n",
    "                state_count[state]+=1\n",
    "            else:\n",
    "                state_count[state]=1\n",
    "                \n",
    "            trained_words.add(observation)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    vocabulary_size = len(trained_words)\n",
    "    \n",
    "    #Use Laplace smoothing with alpha\n",
    "    for k,v in state_observation_count.items():\n",
    "        estimated_emission_parameters[k] = v/(state_count[k[1]]+k_value)\n",
    "        estimated_emission_parameters[(\"#UNK#\",k[1])] = (k_value+alpha)/( (state_count[k[1]]+k_value) + alpha*vocabulary_size)\n",
    "    \n",
    "    return estimated_emission_parameters, list(trained_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c039f48",
   "metadata": {},
   "source": [
    "#### Define function that estimates transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824838a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_parameters(training_set):\n",
    "    #Break the training set into sentences, where each sentence contains its states\n",
    "    training_set = sentence_creator_states(training_set)\n",
    "    state_count = {}\n",
    "    state_transition_count = {}\n",
    "    estimated_transition_parameters = {}\n",
    "    \n",
    "    #For each sentence (list of states)\n",
    "    for sentence in training_set:\n",
    "        #For each state in the sentence\n",
    "        for i in range(len(sentence)):\n",
    "            #Compute the counts up until the STOP state\n",
    "            if(i!=len(sentence)-1):\n",
    "                #Increment count(y_i-1 -> y_i)\n",
    "                if (sentence[i], sentence[i+1]) in state_transition_count:\n",
    "                    state_transition_count[(sentence[i], sentence[i+1])]+=1\n",
    "                else:\n",
    "                    state_transition_count[(sentence[i], sentence[i+1])]=1\n",
    "\n",
    "                #Increment count(y_i-1)\n",
    "                if sentence[i] in state_count:\n",
    "                    state_count[sentence[i]]+=1\n",
    "                else:\n",
    "                    state_count[sentence[i]]=1\n",
    "                    \n",
    "    #For each y_i|y_i-1, calculate count(y_i-1 -> y_i)/count(y_i-1)\n",
    "    for k,v in state_transition_count.items():\n",
    "        estimated_transition_parameters[k] = v/state_count[k[0]]\n",
    "    \n",
    "    return estimated_transition_parameters, list(state_count.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85beb0f",
   "metadata": {},
   "source": [
    "# Part 2.2: Implement the Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce56fa",
   "metadata": {},
   "source": [
    "#### Define function that implements the viterbi algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f9de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vertibi_algo(sentence_original, all_states_original, trained_words_original, emission_parameters_original, transition_parameters_original):\n",
    "    #Create deep copies\n",
    "    sentence = copy.deepcopy(sentence_original)\n",
    "    all_states = copy.deepcopy(all_states_original)\n",
    "    trained_words = copy.deepcopy(trained_words_original)\n",
    "    emission_parameters = copy.deepcopy(emission_parameters_original)\n",
    "    transition_parameters = copy.deepcopy(transition_parameters_original)\n",
    "    all_states.append('STOP')\n",
    "    \n",
    "    #Pad each sentence to account for position 0 (start) and position n+1 (stop)\n",
    "    sentence.insert(0, \"padding\")\n",
    "    sentence.append(\"padding\")\n",
    "    \n",
    "    #Initialise a score matrix to store the highest scoring paths \n",
    "    #from START to state u at position j\n",
    "    #index by [position,state]\n",
    "    #number of states T\n",
    "    #number of positions 0 to n+1\n",
    "    #dimensions: number of positions x number of states\n",
    "    score_matrix = [[0 for _ in range(len(all_states))] for _ in range(len(sentence))] \n",
    "    \n",
    "    #FORWARD PASS\n",
    "    #0. Map each state to an index number\n",
    "    all_states_map = {k:v for v,k in enumerate(all_states)}\n",
    "    \n",
    "    #1. Initialisation Step\n",
    "    for state in all_states:\n",
    "        if(state=='START'):\n",
    "            score_matrix[0][all_states_map[state]] = 1\n",
    "        else:\n",
    "            score_matrix[0][all_states_map[state]] = 0\n",
    "            \n",
    "    #2. Forward Pass from j=0 to j=n-1 (inclusive)\n",
    "    #0 1 n-1 n n+1\n",
    "    #len = 5\n",
    "    #len-2 = 3\n",
    "    #For each position j from 0 to n-1\n",
    "    for position in range(len(sentence)-2):\n",
    "        #For each state u belonging to T except for START and STOP\n",
    "        for state in all_states:\n",
    "            if(state=='START' or state=='STOP'):\n",
    "                continue\n",
    "            score = 0\n",
    "            #For each prev state v belonging to T\n",
    "            for prev_state in all_states:\n",
    "                temp = 0\n",
    "                #If the word in j+1 position appears in the training set\n",
    "                if(sentence[position+1] in trained_words):\n",
    "                    #If the emission and transition has been trained before\n",
    "                    if((sentence[position+1],state) in emission_parameters.keys() and (prev_state,state) in transition_parameters.keys()):\n",
    "                        #Calculate: score of prev_state v in position j * emission prob of observation from curr_state u * transiton prob of prev_state v to curr_state u\n",
    "                        temp = score_matrix[position][all_states_map[prev_state]]*emission_parameters[(sentence[position+1],state)]*transition_parameters[(prev_state,state)]\n",
    "                else:\n",
    "                    #If the emission and transition has been trained before\n",
    "                    if((\"#UNK#\",state) in emission_parameters.keys() and (prev_state,state) in transition_parameters.keys()):\n",
    "                        #Calculate: score of prev_state v in position j * emission prob of #UNK# from curr_state u * transiton prob of prev_state v to curr_state u\n",
    "                        temp = score_matrix[position][all_states_map[prev_state]]*emission_parameters[(\"#UNK#\",state)]*transition_parameters[(prev_state,state)]\n",
    "                #Store the score of the highest scoring path (max v) from START to this node (position j+1, state u) \n",
    "                if(temp>score):\n",
    "                    score = temp\n",
    "                    score_matrix[position+1][all_states_map[state]] = score\n",
    "                    \n",
    "    #3. Final Step\n",
    "    #0 1 n-1 n n+1\n",
    "    #len = 5\n",
    "    #len-2 = 3\n",
    "    #len-1 = 4\n",
    "    score = 0\n",
    "    #For each prev state v belonging to T\n",
    "    for prev_state in all_states:\n",
    "        temp = 0\n",
    "        #If the transition has been trained before\n",
    "        if((prev_state,'STOP') in transition_parameters.keys()):\n",
    "            #Calculate: score of prev_state v in position n * transiton prob of prev_state v to STOP state\n",
    "            temp = score_matrix[len(sentence)-2][all_states_map[prev_state]]*transition_parameters[(prev_state,'STOP')]\n",
    "\n",
    "        #Store the score of the highest scoring path (max v) from START to STOP \n",
    "        if(temp>score):\n",
    "            score = temp\n",
    "            score_matrix[len(sentence)-1][all_states_map['STOP']] = score\n",
    "            \n",
    "    #############################################################################################\n",
    "    \n",
    "    #Backward Pass\n",
    "    #Define optimal_y: a list to store the optimal y found in the backwards pass\n",
    "    optimal_y = [None for _ in range(len(sentence))]\n",
    "    \n",
    "    #len(sentence)-1: last index\n",
    "    #Calculate y_n*\n",
    "    #at position n\n",
    "    score = 0\n",
    "    for state in all_states:\n",
    "        temp = 0\n",
    "        if(state=='START' or state=='STOP'):\n",
    "            continue\n",
    "        #If the transition has been trained before\n",
    "        if((state,'STOP') in transition_parameters.keys()):\n",
    "            temp = score_matrix[len(sentence)-2][all_states_map[state]] * transition_parameters[(state,'STOP')]\n",
    "        if(temp>score):\n",
    "            score = temp\n",
    "            optimal_y[len(sentence)-2] = state\n",
    "\n",
    "    #Calculate y_j*\n",
    "    #from position n-1 to position 1 (inclusive)\n",
    "    for j in range(len(sentence)-3,0,-1):\n",
    "        score = 0\n",
    "        for state in all_states:\n",
    "            temp = 0\n",
    "            if(state=='START' or state=='STOP'):\n",
    "                continue\n",
    "            #If the transition has been trained before: from this state to the optimal state found in the next position\n",
    "            if((state,optimal_y[j+1]) in transition_parameters.keys()):\n",
    "                temp = score_matrix[j][all_states_map[state]] * transition_parameters[(state,optimal_y[j+1])] \n",
    "            if(temp>score):\n",
    "                score = temp\n",
    "                optimal_y[j] = state\n",
    "                \n",
    "    #############################################################################################\n",
    "    \n",
    "    #Clean up\n",
    "    \n",
    "    #Remove the added padding to the sentence\n",
    "    sentence.pop(0)\n",
    "    sentence.pop(len(sentence)-1)\n",
    "    \n",
    "    #Remove position 0 and position n+1\n",
    "    optimal_y.pop(0)\n",
    "    optimal_y.pop(len(optimal_y)-1)\n",
    "    \n",
    "    #Join the sentence with its predicted states\n",
    "    tagged_sentence = [f\"{sentence[i]} {optimal_y[i]}\" for i in range(len(sentence))]\n",
    "    \n",
    "    return tagged_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb9165f",
   "metadata": {},
   "source": [
    "#### Define helper function that creates a list of lists where each list contains only the observations of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b29b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a list of lists where each list contains only the observations of a sentence\n",
    "def sentence_creator_observations(original):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for i in original:\n",
    "        if i!='':\n",
    "            sentence.append(i)\n",
    "        else:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            \n",
    "    if len(sentence)!=0:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053959ec",
   "metadata": {},
   "source": [
    "## Train and Evaluate with ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d198fd",
   "metadata": {},
   "source": [
    "#### Read ES Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d2a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_ES_train = os.path.join(os.getcwd(), 'Data', 'ES', 'train')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_ES_train, 'r', encoding='utf-8') as file:\n",
    "    file_contents_ES_train = file.readlines()\n",
    "    \n",
    "#Convert to training set\n",
    "es_training_set = [w.strip() for w in file_contents_ES_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06343939",
   "metadata": {},
   "source": [
    "#### Learn ES emission and transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5626b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the parameters using the training set\n",
    "estimated_emission_parameters,trained_words = estimate_emission_parameters(es_training_set)\n",
    "estimated_transition_parameters, all_states = estimate_transition_parameters(es_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c3ee6",
   "metadata": {},
   "source": [
    "#### Read ES dev.in Dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e07c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_ES_devin = os.path.join(os.getcwd(), 'Data', 'ES', 'dev.in')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_ES_devin, 'r', encoding='utf-8') as file:\n",
    "    file_contents_ES_devin = file.readlines()\n",
    "    \n",
    "es_devin = [w.strip() for w in file_contents_ES_devin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0124111",
   "metadata": {},
   "source": [
    "#### Convert ES dev.in into a list of lists where each list a sentence of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ea83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_devin = sentence_creator_observations(es_devin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d87054",
   "metadata": {},
   "source": [
    "#### Run the Vertibi Algorithm on each sentence of ES dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689fab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "for i in range(len(es_devin)):\n",
    "    es_devin[i] = vertibi_algo(es_devin[i], all_states, trained_words, estimated_emission_parameters, estimated_transition_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3055d",
   "metadata": {},
   "source": [
    "#### Join all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bc1d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_devin_predicted = []\n",
    "for sentence in es_devin:\n",
    "    for i in range(len(sentence)):\n",
    "        if(i==len(sentence)-1):\n",
    "            es_devin_predicted.append(sentence[i])\n",
    "            es_devin_predicted.append('')\n",
    "        else:\n",
    "            es_devin_predicted.append(sentence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ba4dd",
   "metadata": {},
   "source": [
    "#### Write to dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bad6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p4_out = os.path.join(os.getcwd(), 'Data', 'ES', 'dev.p4.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dad04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath_dev_p4_out, 'w', encoding='utf-8') as file:\n",
    "    for line in es_devin_predicted:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59d3c3",
   "metadata": {},
   "source": [
    "#### Compare dev.p4.out with dev.out for ES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e870bfd5",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p4.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3b81b9b",
   "metadata": {},
   "source": [
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 217\n",
    "\n",
    "#Correct Entity : 138\n",
    "Entity  precision: 0.6359\n",
    "Entity  recall: 0.6026\n",
    "Entity  F: 0.6188\n",
    "\n",
    "#Correct Sentiment : 108\n",
    "Sentiment  precision: 0.4977\n",
    "Sentiment  recall: 0.4716\n",
    "Sentiment  F: 0.4843"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1479760",
   "metadata": {},
   "source": [
    "## Train and Evaluate with RU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfd66a",
   "metadata": {},
   "source": [
    "#### Read RU Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a49744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_RU_train = os.path.join(os.getcwd(), 'Data', 'RU', 'train')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_RU_train, 'r', encoding='utf-8') as file:\n",
    "    file_contents_RU_train = file.readlines()\n",
    "    \n",
    "#Convert to training set\n",
    "ru_training_set = [w.strip() for w in file_contents_RU_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c8e67",
   "metadata": {},
   "source": [
    "#### Learn RU emission and transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76c5abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the parameters using the training set\n",
    "estimated_emission_parameters,trained_words = estimate_emission_parameters(ru_training_set)\n",
    "estimated_transition_parameters, all_states = estimate_transition_parameters(ru_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3637e",
   "metadata": {},
   "source": [
    "#### Read RU dev.in Dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f49ad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_RU_devin = os.path.join(os.getcwd(), 'Data', 'RU', 'dev.in')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_RU_devin, 'r', encoding='utf-8') as file:\n",
    "    file_contents_RU_devin = file.readlines()\n",
    "    \n",
    "ru_devin = [w.strip() for w in file_contents_RU_devin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0245f",
   "metadata": {},
   "source": [
    "#### Convert RU dev.in into a list of lists where each list a sentence of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e1f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_devin = sentence_creator_observations(ru_devin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e1160",
   "metadata": {},
   "source": [
    "#### Run the Vertibi Algorithm on each sentence of RU dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898e830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "for i in range(len(ru_devin)):\n",
    "    ru_devin[i] = vertibi_algo(ru_devin[i], all_states, trained_words, estimated_emission_parameters, estimated_transition_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1b30b",
   "metadata": {},
   "source": [
    "#### Join all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dc55e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_devin_predicted = []\n",
    "for sentence in ru_devin:\n",
    "    for i in range(len(sentence)):\n",
    "        if(i==len(sentence)-1):\n",
    "            ru_devin_predicted.append(sentence[i])\n",
    "            ru_devin_predicted.append('')\n",
    "        else:\n",
    "            ru_devin_predicted.append(sentence[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae55e2",
   "metadata": {},
   "source": [
    "#### Write to dev.p4.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed1f0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p4_out = os.path.join(os.getcwd(), 'Data', 'RU', 'dev.p4.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3e26c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath_dev_p4_out, 'w', encoding='utf-8') as file:\n",
    "    for line in ru_devin_predicted:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bed867",
   "metadata": {},
   "source": [
    "#### Compare dev.p4.out with dev.out for RU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97436085",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p4.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a8c0da",
   "metadata": {},
   "source": [
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 312\n",
    "\n",
    "#Correct Entity : 189\n",
    "Entity  precision: 0.6058\n",
    "Entity  recall: 0.4859\n",
    "Entity  F: 0.5392\n",
    "\n",
    "#Correct Sentiment : 136\n",
    "Sentiment  precision: 0.4359\n",
    "Sentiment  recall: 0.3496\n",
    "Sentiment  F: 0.3880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7144f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
