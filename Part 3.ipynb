{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9ec8c3",
   "metadata": {},
   "source": [
    "# Part 3: Implement an algorithm to find the k-th best output sequences (25 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b235250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import heapq\n",
    "from project_utils import estimate_emission_parameters\n",
    "from project_utils import sentence_creator_states\n",
    "from project_utils import estimate_transition_parameters\n",
    "from project_utils import sentence_creator_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed6d78",
   "metadata": {},
   "source": [
    "### Define the modified viterbi function to compute k best sequences using the forward pass with beam search, Comparison of sequences is done by joint probability, the products of transition and emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3ef70",
   "metadata": {},
   "source": [
    "#### Function that keeps only the k best sequences after every position is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fdd2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_vertibi_algo(sentence_original, all_states_original, trained_words_original, emission_parameters_original, transition_parameters_original, k_sequences):\n",
    "    #Create deep copies\n",
    "    sentence = copy.deepcopy(sentence_original)\n",
    "    all_states = copy.deepcopy(all_states_original)\n",
    "    trained_words = copy.deepcopy(trained_words_original)\n",
    "    emission_parameters = copy.deepcopy(emission_parameters_original)\n",
    "    transition_parameters = copy.deepcopy(transition_parameters_original)\n",
    "    all_states.append('STOP')\n",
    "    \n",
    "    #1. Initialisation Step: Initialise a heap to store the sequences containing \n",
    "    #   only the score of the START state and the START state itself\n",
    "    top_sequences = [(1, ['START'])]\n",
    "    \n",
    "    # Initialise a variable to keep track of the position in seq_path\n",
    "    seq_path_position = 0\n",
    "    \n",
    "    #2. Forward Pass from the first word to nth word (inclusive)\n",
    "    # Recursive step of beam search \n",
    "    # For each position j from the first word to nth word (inclusive)\n",
    "    for position in range(0, len(sentence)):\n",
    "\n",
    "        # Initialise a list to hold all temp sequences\n",
    "        temp_sequences = []\n",
    "\n",
    "        # For each sequence score and sequence path in the current top sequences\n",
    "        for seq_score, seq_path in top_sequences:\n",
    "\n",
    "            # For each state u belonging to T at the current position except for START and STOP\n",
    "            for state in all_states:\n",
    "                if(state=='START' or state=='STOP'):\n",
    "                    continue\n",
    "\n",
    "                # If the word appears in the training set\n",
    "                if(sentence[position] in trained_words):\n",
    "                    # If the emission and transition has been trained before\n",
    "                    if((sentence[position],state) in emission_parameters.keys() and (seq_path[seq_path_position],state) in transition_parameters.keys()):\n",
    "                        # Calculate the extended sequence score: \n",
    "                        # score of current seq (product of transitions and emissions in seq) *\n",
    "                        # transiton prob of latest state in seq to curr_state *\n",
    "                        # emission prob of observation from curr_state \n",
    "                        extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]*emission_parameters[(sentence[position],state)]\n",
    "\n",
    "                        # Extend the sequence path\n",
    "                        extended_seq_path = seq_path + [state]\n",
    "\n",
    "                        # Push the extended sequence into the heap\n",
    "                        extended_seq = (extended_seq_score,extended_seq_path)\n",
    "                        heapq.heappush(temp_sequences, extended_seq)\n",
    "                else:\n",
    "                    # If the emission and transition has been trained before\n",
    "                    if((\"#UNK#\",state) in emission_parameters.keys() and (seq_path[seq_path_position],state) in transition_parameters.keys()):\n",
    "                        # Calculate the extended sequence score: \n",
    "                        # score of current seq (product of transitions and emissions in seq) *\n",
    "                        # transiton prob of latest state in seq to curr_state *\n",
    "                        # emission prob of #UNK# from curr_state\n",
    "                        extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]*emission_parameters[(\"#UNK#\",state)]\n",
    "\n",
    "                        # Extend the sequence path\n",
    "                        extended_seq_path = seq_path + [state]\n",
    "\n",
    "                        # Push the extended sequence into the heap\n",
    "                        extended_seq = (extended_seq_score,extended_seq_path)\n",
    "                        heapq.heappush(temp_sequences, extended_seq)    \n",
    "\n",
    "\n",
    "        #Prune step: Get the k top_sequences from this pass only\n",
    "        top_sequences = heapq.nlargest(k_sequences, temp_sequences)\n",
    "        #Increase the seq_path_position counter\n",
    "        seq_path_position+=1\n",
    "        \n",
    "    #3. Recursion step of beam search at the last word\n",
    "    # Initialise a list to hold all temp sequences\n",
    "    temp_sequences = []\n",
    "\n",
    "    # For each sequence score and sequence path in the current top sequences\n",
    "    for seq_score, seq_path in top_sequences:\n",
    "\n",
    "        # If the transition has been trained before\n",
    "        if((seq_path[seq_path_position],\"STOP\") in transition_parameters.keys()):\n",
    "            # Calculate the extended sequence score: \n",
    "            # score of current seq (product of transitions and emissions in seq) *\n",
    "            # transiton prob of latest state in seq to STOP state  \n",
    "            extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]\n",
    "\n",
    "            # Extend the sequence path\n",
    "            extended_seq_path = seq_path + [\"STOP\"]\n",
    "\n",
    "            # Push the extended sequence into the heap\n",
    "            extended_seq = (extended_seq_score,extended_seq_path)\n",
    "            heapq.heappush(temp_sequences, extended_seq)\n",
    "\n",
    "\n",
    "    #Prune step: Get the final top 8 sequences \n",
    "    top_sequences = heapq.nlargest(k_sequences, temp_sequences)\n",
    "    \n",
    "    #Check if top_sequences is empty. \n",
    "    #If it is, expand the beam search to include all possible sequences\n",
    "    #As some sequences that were potential optimal candidates may have gotten dropped earlier on\n",
    "    #as they did not have a high score in the earlier positions\n",
    "    if(len(top_sequences)==0):\n",
    "        top_sequences = modified_vertibi_algo_allseq(sentence_original, all_states_original, trained_words_original, emission_parameters_original, transition_parameters_original, k_sequences)\n",
    "        return top_sequences\n",
    "    \n",
    "    #In the case where the number of sequences generated is less than the number of sequences\n",
    "    #Pad the sequences using the last i.e. worst performing sequence\n",
    "    while(len(top_sequences)<k_sequences):\n",
    "        top_sequences.append(copy.deepcopy(top_sequences[len(top_sequences)-1]))\n",
    "    \n",
    "    # Clean up the top_sequences to get only the predicted states\n",
    "    # for the top k sequences\n",
    "    for i in range(len(top_sequences)):\n",
    "        temp = top_sequences[i][1]\n",
    "        temp.pop(0)\n",
    "        temp.pop(len(temp)-1)\n",
    "        top_sequences[i] = temp\n",
    "        \n",
    "    # Combine the sentences with its predicted states\n",
    "    for i in range(len(top_sequences)):\n",
    "        temp = [f\"{sentence[j]} {top_sequences[i][j]}\" for j in range(len(sentence))]\n",
    "        top_sequences[i] = temp\n",
    "        \n",
    "    return top_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9744db",
   "metadata": {},
   "source": [
    "#### Function that keeps all sequences after every position is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e640e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_vertibi_algo_allseq(sentence_original, all_states_original, trained_words_original, emission_parameters_original, transition_parameters_original, k_sequences):\n",
    "    #Create deep copies\n",
    "    sentence = copy.deepcopy(sentence_original)\n",
    "    all_states = copy.deepcopy(all_states_original)\n",
    "    trained_words = copy.deepcopy(trained_words_original)\n",
    "    emission_parameters = copy.deepcopy(emission_parameters_original)\n",
    "    transition_parameters = copy.deepcopy(transition_parameters_original)\n",
    "    all_states.append('STOP')\n",
    "    \n",
    "    #1. Initialisation Step: Initialise a heap to store the sequences containing \n",
    "    #   only the score of the START state and the START state itself\n",
    "    all_sequences = [(1, ['START'])]\n",
    "    \n",
    "    # Initialise a variable to keep track of the position in seq_path\n",
    "    seq_path_position = 0\n",
    "    \n",
    "    #2. Forward Pass from the first word to nth word (inclusive)\n",
    "    # Recursive step of beam search \n",
    "    # For each position j from the first word to nth word (inclusive)\n",
    "    for position in range(0, len(sentence)):\n",
    "\n",
    "        # Initialise a list to hold all temp sequences\n",
    "        temp_sequences = []\n",
    "\n",
    "        # For each sequence score and sequence path in the current top sequences\n",
    "        for seq_score, seq_path in all_sequences:\n",
    "\n",
    "            # For each state u belonging to T at the current position except for START and STOP\n",
    "            for state in all_states:\n",
    "                if(state=='START' or state=='STOP'):\n",
    "                    continue\n",
    "\n",
    "                # If the word appears in the training set\n",
    "                if(sentence[position] in trained_words):\n",
    "                    # If the emission and transition has been trained before\n",
    "                    if((sentence[position],state) in emission_parameters.keys() and (seq_path[seq_path_position],state) in transition_parameters.keys()):\n",
    "                        # Calculate the extended sequence score: \n",
    "                        # score of current seq (product of transitions and emissions in seq) *\n",
    "                        # transiton prob of latest state in seq to curr_state *\n",
    "                        # emission prob of observation from curr_state \n",
    "                        extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]*emission_parameters[(sentence[position],state)]\n",
    "\n",
    "                        # Extend the sequence path\n",
    "                        extended_seq_path = seq_path + [state]\n",
    "\n",
    "                        # Push the extended sequence into the heap\n",
    "                        extended_seq = (extended_seq_score,extended_seq_path)\n",
    "                        heapq.heappush(temp_sequences, extended_seq)\n",
    "                else:\n",
    "                    # If the emission and transition has been trained before\n",
    "                    if((\"#UNK#\",state) in emission_parameters.keys() and (seq_path[seq_path_position],state) in transition_parameters.keys()):\n",
    "                        # Calculate the extended sequence score: \n",
    "                        # score of current seq (product of transitions and emissions in seq) *\n",
    "                        # transiton prob of latest state in seq to curr_state *\n",
    "                        # emission prob of #UNK# from curr_state\n",
    "                        extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]*emission_parameters[(\"#UNK#\",state)]\n",
    "\n",
    "                        # Extend the sequence path\n",
    "                        extended_seq_path = seq_path + [state]\n",
    "\n",
    "                        # Push the extended sequence into the heap\n",
    "                        extended_seq = (extended_seq_score,extended_seq_path)\n",
    "                        heapq.heappush(temp_sequences, extended_seq)    \n",
    "\n",
    "\n",
    "        all_sequences = temp_sequences\n",
    "        #Increase the seq_path_position counter\n",
    "        seq_path_position+=1\n",
    "        \n",
    "    #3. Recursion step of beam search at the last word\n",
    "    # Initialise a list to hold all temp sequences\n",
    "    temp_sequences = []\n",
    "\n",
    "    # For each sequence score and sequence path in the current sequences found\n",
    "    for seq_score, seq_path in all_sequences:\n",
    "\n",
    "        # If the transition has been trained before\n",
    "        if((seq_path[seq_path_position],\"STOP\") in transition_parameters.keys()):\n",
    "            # Calculate the extended sequence score: \n",
    "            # score of current seq (product of transitions and emissions in seq) *\n",
    "            # transiton prob of latest state in seq to STOP state  \n",
    "            extended_seq_score = seq_score*transition_parameters[(seq_path[seq_path_position],state)]\n",
    "\n",
    "            # Extend the sequence path\n",
    "            extended_seq_path = seq_path + [\"STOP\"]\n",
    "\n",
    "            # Push the extended sequence into the heap\n",
    "            extended_seq = (extended_seq_score,extended_seq_path)\n",
    "            heapq.heappush(temp_sequences, extended_seq)\n",
    "\n",
    "\n",
    "    #Prune step: Get the final top 8 sequences from all sequences \n",
    "    top_sequences = heapq.nlargest(k_sequences, temp_sequences)\n",
    "    \n",
    "    #If no prediction is available for this sentence, assign a None value for all the states\n",
    "    if(len(top_sequences)==0):\n",
    "        top_sequences = [(0, ['START'] + [None]*len(sentence) + ['STOP'])]\n",
    "    \n",
    "    #In the case where the number of sequences generated is less than the number of sequences\n",
    "    #Pad the sequences using the last i.e. worst performing sequence\n",
    "    while(len(top_sequences)<k_sequences):\n",
    "        top_sequences.append(copy.deepcopy(top_sequences[len(top_sequences)-1]))\n",
    "    \n",
    "    # Clean up the top_sequences to get only the predicted states\n",
    "    # for the top k sequences\n",
    "    for i in range(len(top_sequences)):\n",
    "        temp = top_sequences[i][1]\n",
    "        temp.pop(0)\n",
    "        temp.pop(len(temp)-1)\n",
    "        top_sequences[i] = temp\n",
    "        \n",
    "    # Combine the sentences with its predicted states\n",
    "    for i in range(len(top_sequences)):\n",
    "        temp = [f\"{sentence[j]} {top_sequences[i][j]}\" for j in range(len(sentence))]\n",
    "        top_sequences[i] = temp\n",
    "        \n",
    "    return top_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac0c96",
   "metadata": {},
   "source": [
    "## Train and Evaluate with ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8e867",
   "metadata": {},
   "source": [
    "#### Read ES Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d782940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_ES_train = os.path.join(os.getcwd(), 'Data', 'ES', 'train')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_ES_train, 'r', encoding='utf-8') as file:\n",
    "    file_contents_ES_train = file.readlines()\n",
    "    \n",
    "#Convert to training set\n",
    "es_training_set = [w.strip() for w in file_contents_ES_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245ed830",
   "metadata": {},
   "source": [
    "#### Learn ES emission and transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d1a7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the parameters using the training set\n",
    "estimated_emission_parameters,trained_words = estimate_emission_parameters(es_training_set)\n",
    "estimated_transition_parameters, all_states = estimate_transition_parameters(es_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dad13b",
   "metadata": {},
   "source": [
    "#### Read ES dev.in Dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b2e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_ES_devin = os.path.join(os.getcwd(), 'Data', 'ES', 'dev.in')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_ES_devin, 'r', encoding='utf-8') as file:\n",
    "    file_contents_ES_devin = file.readlines()\n",
    "    \n",
    "es_devin = [w.strip() for w in file_contents_ES_devin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767a718",
   "metadata": {},
   "source": [
    "#### Convert ES dev.in into a list of lists where each list a sentence of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e82335a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_devin = sentence_creator_observations(es_devin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9d9f9",
   "metadata": {},
   "source": [
    "#### Run the Modified Vertibi Algorithm on each sentence of ES dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3d608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sequences = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d90fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "for i in range(len(es_devin)):\n",
    "    es_devin[i] = modified_vertibi_algo(es_devin[i], all_states, trained_words, estimated_emission_parameters, estimated_transition_parameters, k_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b54199",
   "metadata": {},
   "source": [
    "#### Join all k sequences together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44d9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_devin_predicted_k_sequences = []\n",
    "# For the k-th best sequence\n",
    "for i in range(k_sequences):\n",
    "    # Define a list to hold the results for the k-th best sequence\n",
    "    k_sequence = []\n",
    "    \n",
    "    # For k-th best sequence of each sentence\n",
    "    for sequences in es_devin:\n",
    "        # For each pair of word and predicted tag in the k-th best sequence of the current sentence\n",
    "        # Add it to k_sequence\n",
    "        for j in range(len(sequences[i])):\n",
    "            # If its the last pair of word and predicted tag, add an empty line behind\n",
    "            if(j==len(sequences[i])-1):\n",
    "                k_sequence.append(sequences[i][j])\n",
    "                k_sequence.append(\"\")\n",
    "            # Add the word and predicted tag\n",
    "            else:\n",
    "                k_sequence.append(sequences[i][j])\n",
    "    \n",
    "    # When this k-th best sequence is done, move to the next k-th best sequence\n",
    "    es_devin_predicted_k_sequences.append(k_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814af91",
   "metadata": {},
   "source": [
    "#### Results for ES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b33e4",
   "metadata": {},
   "source": [
    "#### Write to dev.p3.2nd.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d6a10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p3_k2_out = os.path.join(os.getcwd(), 'Data', 'ES', 'dev.p3.2nd.out')\n",
    "with open(filepath_dev_p3_k2_out, 'w', encoding='utf-8') as file:\n",
    "    for line in es_devin_predicted_k_sequences[1]:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae9dc43a",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p3.2nd.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d10ae83",
   "metadata": {},
   "source": [
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 454\n",
    "\n",
    "#Correct Entity : 119\n",
    "Entity  precision: 0.2621\n",
    "Entity  recall: 0.5197\n",
    "Entity  F: 0.3485\n",
    "\n",
    "#Correct Sentiment : 70\n",
    "Sentiment  precision: 0.1542\n",
    "Sentiment  recall: 0.3057\n",
    "Sentiment  F: 0.2050"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19babc69",
   "metadata": {},
   "source": [
    "#### Write to dev.p3.8th.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c41ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p3_k8_out = os.path.join(os.getcwd(), 'Data', 'ES', 'dev.p3.8th.out')\n",
    "with open(filepath_dev_p3_k8_out, 'w', encoding='utf-8') as file:\n",
    "    for line in es_devin_predicted_k_sequences[7]:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abea6bcb",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p3.8th.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8db959e5",
   "metadata": {},
   "source": [
    "#Entity in gold data: 229\n",
    "#Entity in prediction: 539\n",
    "\n",
    "#Correct Entity : 106\n",
    "Entity  precision: 0.1967\n",
    "Entity  recall: 0.4629\n",
    "Entity  F: 0.2760\n",
    "\n",
    "#Correct Sentiment : 63\n",
    "Sentiment  precision: 0.1169\n",
    "Sentiment  recall: 0.2751\n",
    "Sentiment  F: 0.1641"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e281fd2",
   "metadata": {},
   "source": [
    "## Train and Evaluate with RU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda5d33",
   "metadata": {},
   "source": [
    "#### Read RU Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d72fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_RU_train = os.path.join(os.getcwd(), 'Data', 'RU', 'train')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_RU_train, 'r', encoding='utf-8') as file:\n",
    "    file_contents_RU_train = file.readlines()\n",
    "    \n",
    "#Convert to training set\n",
    "ru_training_set = [w.strip() for w in file_contents_RU_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62719bd2",
   "metadata": {},
   "source": [
    "#### Learn RU emission and transition parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31944b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the parameters using the training set\n",
    "estimated_emission_parameters,trained_words = estimate_emission_parameters(ru_training_set)\n",
    "estimated_transition_parameters, all_states = estimate_transition_parameters(ru_training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc80e5",
   "metadata": {},
   "source": [
    "#### Read RU dev.in Dataset for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc26d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_RU_devin = os.path.join(os.getcwd(), 'Data', 'RU', 'dev.in')\n",
    "\n",
    "#Read the file contents\n",
    "with open(filepath_RU_devin, 'r', encoding='utf-8') as file:\n",
    "    file_contents_RU_devin = file.readlines()\n",
    "    \n",
    "ru_devin = [w.strip() for w in file_contents_RU_devin]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce62c9e",
   "metadata": {},
   "source": [
    "#### Convert RU dev.in into a list of lists where each list a sentence of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c60c52a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_devin = sentence_creator_observations(ru_devin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403578a",
   "metadata": {},
   "source": [
    "#### Run the Modified Vertibi Algorithm on each sentence of ES dev.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a18cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_sequences = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72782a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each sentence\n",
    "for i in range(len(ru_devin)):\n",
    "    ru_devin[i] = modified_vertibi_algo(ru_devin[i], all_states, trained_words, estimated_emission_parameters, estimated_transition_parameters, k_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e394c1e",
   "metadata": {},
   "source": [
    "#### Join all k sequences together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "490962c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_devin_predicted_k_sequences = []\n",
    "# For the k-th best sequence\n",
    "for i in range(k_sequences):\n",
    "    # Define a list to hold the results for the k-th best sequence\n",
    "    k_sequence = []\n",
    "    \n",
    "    # For k-th best sequence of each sentence\n",
    "    for sequences in ru_devin:\n",
    "        # For each pair of word and predicted tag in the k-th best sequence of the current sentence\n",
    "        # Add it to k_sequence\n",
    "        for j in range(len(sequences[i])):\n",
    "            # If its the last pair of word and predicted tag, add an empty line behind\n",
    "            if(j==len(sequences[i])-1):\n",
    "                k_sequence.append(sequences[i][j])\n",
    "                k_sequence.append(\"\")\n",
    "            # Add the word and predicted tag\n",
    "            else:\n",
    "                k_sequence.append(sequences[i][j])\n",
    "    \n",
    "    # When this k-th best sequence is done, move to the next k-th best sequence\n",
    "    ru_devin_predicted_k_sequences.append(k_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe975f47",
   "metadata": {},
   "source": [
    "#### Results for RU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f6fbc6",
   "metadata": {},
   "source": [
    "#### Write to dev.p3.2nd.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5926324",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p3_k2_out = os.path.join(os.getcwd(), 'Data', 'RU', 'dev.p3.2nd.out')\n",
    "with open(filepath_dev_p3_k2_out, 'w', encoding='utf-8') as file:\n",
    "    for line in ru_devin_predicted_k_sequences[1]:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "837047f9",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p3.2nd.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6c05e8e",
   "metadata": {},
   "source": [
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 677\n",
    "\n",
    "#Correct Entity : 198\n",
    "Entity  precision: 0.2925\n",
    "Entity  recall: 0.5090\n",
    "Entity  F: 0.3715\n",
    "\n",
    "#Correct Sentiment : 123\n",
    "Sentiment  precision: 0.1817\n",
    "Sentiment  recall: 0.3162\n",
    "Sentiment  F: 0.2308"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d925ccf7",
   "metadata": {},
   "source": [
    "#### Write to dev.p3.8th.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea80fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dev_p3_k8_out = os.path.join(os.getcwd(), 'Data', 'RU', 'dev.p3.8th.out')\n",
    "with open(filepath_dev_p3_k8_out, 'w', encoding='utf-8') as file:\n",
    "    for line in ru_devin_predicted_k_sequences[7]:\n",
    "        file.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c38ffefa",
   "metadata": {},
   "source": [
    "python evalResult.py dev.out dev.p3.8th.out"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b57c3a7d",
   "metadata": {},
   "source": [
    "#Entity in gold data: 389\n",
    "#Entity in prediction: 779\n",
    "\n",
    "#Correct Entity : 176\n",
    "Entity  precision: 0.2259\n",
    "Entity  recall: 0.4524\n",
    "Entity  F: 0.3014\n",
    "\n",
    "#Correct Sentiment : 101\n",
    "Sentiment  precision: 0.1297\n",
    "Sentiment  recall: 0.2596\n",
    "Sentiment  F: 0.1729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac38181",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
